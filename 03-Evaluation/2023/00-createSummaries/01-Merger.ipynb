{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c87847c",
   "metadata": {},
   "source": [
    "# Merge Facts from Submitted Runs\n",
    "\n",
    "This notebook selects runs to include from `submissions.csv` via the `priority` column and merges facts from all relevant runs into a common set of files for each request-id/event-day pair. We will use these files to de-duplicate submitted facts in the next step. For each included run, we group facts by request-id (again, event-day pair) and copy these facts to a single file all facts from all included submissions that are produced for this request-id. After running this script, we have an `event-day` directory with one file for each request-id, and each file contains all submitted facts from all included runs for that event-day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cac04663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import bert_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49618168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "033ae515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975f5267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8f19f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_df = pd.read_csv(\"submissions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2c3f6a5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crisisfacts\n",
      "\t 1, 1\n",
      "umd_hcil\n",
      "\t 1\n"
     ]
    }
   ],
   "source": [
    "all_runs_to_include = set()\n",
    "for team,group in run_data_df.groupby(\"team\"):\n",
    "    print(team)\n",
    "    print(\"\\t\", \", \".join(group[\"priority\"].apply(str)))\n",
    "    \n",
    "    runs_to_include = group[group[\"priority\"] <= 2].sort_values(by=\"priority\", ascending=False).head(2)\n",
    "    all_runs_to_include = all_runs_to_include.union(runs_to_include[\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a483365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Thesis_Retriver.gz', 'baseline.v1.gz', 'llama.gz'},\n",
       " 'submissions\\\\Thesis_Retriver.gz')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "65e3854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_to_runtag = {row[\"filename\"]:row[\"runtag\"] for idx,row in run_data_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a620b066",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thesis_Retriver'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_to_runtag[\"Thesis_Retriver.gz\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96bbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "38a6c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"event-days\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24711301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e7b8d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a54020b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submissions\\Thesis_Retriver.gz Thesis_Retriver\n",
      "\t SKIPPING\n"
     ]
    }
   ],
   "source": [
    "for submission_file in glob.glob(\"submissions\\*.gz\"):\n",
    "    submission_file = submission_file.split(\"\\\\\")[-1]\n",
    "    print(submission_file, filename_to_runtag[submission_file.split(\"\\\\\")[-1]])\n",
    "    \n",
    "    if not submission_file in all_runs_to_include:\n",
    "        print(\"\\t\", \"SKIPPING\")\n",
    "        continue\n",
    "    \n",
    "    runtag = filename_to_runtag[submission_file]\n",
    "    \n",
    "    with gzip.open(submission_file, \"rb\") as in_file:\n",
    "        rows = []\n",
    "        for line_ in in_file:\n",
    "            line = line_.decode(\"utf8\")\n",
    "            fact = json.loads(line)\n",
    "            \n",
    "            rows.append(fact)\n",
    "            \n",
    "        this_run_df = pd.DataFrame(rows)\n",
    "        for requestId,group in this_run_df.groupby(\"requestID\"):\n",
    "            new_group_df = group.sort_values(by=\"unixTimestamp\")\n",
    "\n",
    "            # Data hygiene to ensure we have non-empty sentences with more than one token\n",
    "            new_group_df[\"tokens\"] = new_group_df[\"factText\"].apply(lambda s: len(tknzr.tokenize(s)))\n",
    "            new_group_df = new_group_df[new_group_df[\"tokens\"] > 1]\n",
    "            new_group_df = new_group_df[new_group_df[\"factText\"].str.len() > 0].copy()\n",
    "            \n",
    "            # Create new fact IDs starting with the request ID, run-tag, and fact number\n",
    "            #. This new fact ID helps us keep track of what submitted facts get combined\n",
    "            #. in the de-duplication step.\n",
    "            new_group_df.index = list(range(0,new_group_df.shape[0]))\n",
    "            new_group_df[\"factID\"] = [\"%s-%s-%04d\" % (requestId,runtag,i) for i in new_group_df.index]\n",
    "            new_group_df[\"runtag\"] = runtag\n",
    "            \n",
    "            with open(\"%s/%s.json\" % (OUTPUT_DIR,requestId), \"a\") as out_file:\n",
    "                [out_file.write(\"%s\\n\" % (json.dumps(r))) for r in new_group_df.to_dict(orient=\"records\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78830671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
