{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c87847c",
   "metadata": {},
   "source": [
    "# Merge Facts from Submitted Runs\n",
    "\n",
    "For every submitted run with evaluation priority of <= 2 (i.e., the top two most important runs submitted by a team), we take the `TOP_K=32` facts from that run and add it to a running list of facts generated for a specific event-day pair.\n",
    "\n",
    "We will use these lists for de-duplication in the next script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac04663",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import gzip\n",
    "import json\n",
    "import bert_score\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49618168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "033ae515",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975f5267",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f19f378",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_data_df = pd.read_csv(\"submissions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3f6a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs_to_include = set()\n",
    "for team,group in run_data_df.groupby(\"team\"):\n",
    "    print(team)\n",
    "    print(\"\\t\", \", \".join(group[\"priority\"].apply(str)))\n",
    "    \n",
    "    runs_to_include = group[group[\"priority\"] <= 2].sort_values(by=\"priority\", ascending=False).head(2)\n",
    "    all_runs_to_include = all_runs_to_include.union(runs_to_include[\"filename\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a483365",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs_to_include"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e3854a",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_to_runtag = {row[\"filename\"]:row[\"runtag\"] for idx,row in run_data_df.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd96bbf5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a6c840",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = \"event-days\"\n",
    "TOP_K = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24711301",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b8d292",
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a54020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for submission_file in glob.glob(\"*.gz\"):\n",
    "    print(submission_file, filename_to_runtag[submission_file])\n",
    "    \n",
    "    if not submission_file in all_runs_to_include:\n",
    "        print(\"\\t\", \"SKIPPING\")\n",
    "        continue\n",
    "    \n",
    "    runtag = filename_to_runtag[submission_file]\n",
    "    \n",
    "    with gzip.open(submission_file, \"rb\") as in_file:\n",
    "        rows = []\n",
    "        for line_ in in_file:\n",
    "            line = line_.decode(\"utf8\")\n",
    "            fact = json.loads(line)\n",
    "            \n",
    "            rows.append(fact)\n",
    "            \n",
    "        this_run_df = pd.DataFrame(rows)\n",
    "        for requestId,group in this_run_df.groupby(\"requestID\"):\n",
    "            new_group_df = group.sort_values(by=\"unixTimestamp\")\n",
    "\n",
    "            # Data hygiene to ensure we have non-empty sentences with more than one token\n",
    "            new_group_df[\"tokens\"] = new_group_df[\"factText\"].apply(lambda s: len(tknzr.tokenize(s)))\n",
    "            new_group_df = new_group_df[new_group_df[\"tokens\"] > 1]\n",
    "            new_group_df = new_group_df[new_group_df[\"factText\"].str.len() > 0].copy()\n",
    "            \n",
    "            new_group_df.index = list(range(0,new_group_df.shape[0]))\n",
    "            new_group_df[\"factID\"] = [\"%s-%s-%04d\" % (requestId,runtag,i) for i in new_group_df.index]\n",
    "            new_group_df[\"runtag\"] = runtag\n",
    "            \n",
    "            with open(\"%s/%s.json\" % (OUTPUT_DIR,requestId), \"a\") as out_file:\n",
    "                [out_file.write(\"%s\\n\" % (json.dumps(r))) for r in new_group_df.to_dict(orient=\"records\")]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78830671",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
