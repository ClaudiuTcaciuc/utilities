{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891c1805",
   "metadata": {},
   "source": [
    "# ROUGE-Style Evaluation\n",
    "\n",
    "__Author__: Cody Buntain (cbuntain@umd.edu)\n",
    "\n",
    "## Description\n",
    "\n",
    "For automated evaluation in CrisisFACTS, we compare participant-system summaries to three additional sources of event summaries:\n",
    "\n",
    "1. Wikipedia - A simple summary of each event, though we expect these summaries are not massively useful for situational awareness, attention support, or decision making.\n",
    "\n",
    "2. ICS 209 Archive - A dataset of real daily hazard reports, gathered from Lise St. Denis. This data comes from a pre-release version of their updated NIMS database.\n",
    "    \n",
    "3. NIST Assessor Summaries - A dataset of event summaries generated by NIST assessors, where CrisisFACTS coordinators asked NIST assessors to identify and timestamp important facts from each event.\n",
    "\n",
    "We use ROUGE score to compare the top-k most important facts from each participant system to each of the above summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44a96521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5ad5a012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torchmetrics in /Users/cbuntain/anaconda3/lib/python3.9/site-packages (0.10.0)\n",
      "Requirement already satisfied: numpy>=1.17.2 in /Users/cbuntain/anaconda3/lib/python3.9/site-packages (from torchmetrics) (1.22.4)\n",
      "Requirement already satisfied: packaging in /Users/cbuntain/anaconda3/lib/python3.9/site-packages (from torchmetrics) (21.3)\n",
      "Requirement already satisfied: torch>=1.3.1 in /Users/cbuntain/.local/lib/python3.9/site-packages (from torchmetrics) (1.12.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/cbuntain/anaconda3/lib/python3.9/site-packages (from torch>=1.3.1->torchmetrics) (4.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/cbuntain/anaconda3/lib/python3.9/site-packages (from packaging->torchmetrics) (3.0.4)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip available: \u001b[0m\u001b[31;49m22.2.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m22.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c556aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text.rouge import ROUGEScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1a5e9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Gold summaries, generated by `00-CreateMultiSummaries` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "535b363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"gold.summaries.json.gz\", \"rb\") as in_file:\n",
    "    summaries = json.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4111b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CrisisFACTs-2022.facts.json\", \"r\") as in_file:\n",
    "    facts = json.load(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21f1c7",
   "metadata": {},
   "source": [
    "We use the CrisisFACTS 2022 fact list from NIST assessors to determine the number of facts *per day*.\n",
    "\n",
    "We use this \"depth\" to take the top most important facts from each participant system for that day. \n",
    "\n",
    "E.g., if a system returns 1000 facts, but the NIST assessor only found 417 facts for that event-day pair, we take the top 417most important facts, as ranked by the participant system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf72124d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrisisFACTS-001 Lilac Wildfire 2017\n",
      "\t CrisisFACTS-001-r3 267\n",
      "\t CrisisFACTS-001-r4 75\n",
      "\t CrisisFACTS-001-r5 14\n",
      "\t CrisisFACTS-001-r6 29\n",
      "\t CrisisFACTS-001-r7 19\n",
      "\t CrisisFACTS-001-r8 5\n",
      "\t CrisisFACTS-001-r9 3\n",
      "\t CrisisFACTS-001-r10 3\n",
      "\t CrisisFACTS-001-r11 2\n",
      "CrisisFACTS-002 Cranston Wildfire 2018\n",
      "\t CrisisFACTS-002-r1 27\n",
      "\t CrisisFACTS-002-r2 10\n",
      "\t CrisisFACTS-002-r3 4\n",
      "\t CrisisFACTS-002-r4 13\n",
      "\t CrisisFACTS-002-r5 7\n",
      "\t CrisisFACTS-002-r6 1\n",
      "CrisisFACTS-003 Holy Wildfire 2018\n",
      "\t CrisisFACTS-003-r5 37\n",
      "\t CrisisFACTS-003-r6 42\n",
      "\t CrisisFACTS-003-r7 39\n",
      "\t CrisisFACTS-003-r8 37\n",
      "\t CrisisFACTS-003-r9 9\n",
      "\t CrisisFACTS-003-r10 17\n",
      "\t CrisisFACTS-003-r11 4\n",
      "CrisisFACTS-004 Hurricane Florence 2018\n",
      "\t CrisisFACTS-004-r8 5\n",
      "\t CrisisFACTS-004-r9 5\n",
      "\t CrisisFACTS-004-r10 2\n",
      "\t CrisisFACTS-004-r11 4\n",
      "\t CrisisFACTS-004-r12 8\n",
      "\t CrisisFACTS-004-r13 15\n",
      "\t CrisisFACTS-004-r14 55\n",
      "\t CrisisFACTS-004-r15 26\n",
      "\t CrisisFACTS-004-r16 14\n",
      "\t CrisisFACTS-004-r17 37\n",
      "\t CrisisFACTS-004-r18 46\n",
      "\t CrisisFACTS-004-r19 3\n",
      "\t CrisisFACTS-004-r20 6\n",
      "\t CrisisFACTS-004-r21 3\n",
      "\t CrisisFACTS-004-r22 3\n",
      "CrisisFACTS-005 2018 Maryland Flood\n",
      "\t CrisisFACTS-005-r3 55\n",
      "\t CrisisFACTS-005-r4 15\n",
      "\t CrisisFACTS-005-r5 3\n",
      "\t CrisisFACTS-005-r6 1\n",
      "CrisisFACTS-006 Saddleridge Wildfire 2019\n",
      "\t CrisisFACTS-006-r4 6\n",
      "\t CrisisFACTS-006-r5 116\n",
      "\t CrisisFACTS-006-r6 12\n",
      "\t CrisisFACTS-006-r7 3\n",
      "CrisisFACTS-007 Hurricane Laura 2020\n",
      "\t CrisisFACTS-007-r13 188\n",
      "\t CrisisFACTS-007-r14 11\n",
      "CrisisFACTS-008 Hurricane Sally 2020\n",
      "\t CrisisFACTS-008-r3 2\n",
      "\t CrisisFACTS-008-r4 12\n",
      "\t CrisisFACTS-008-r5 14\n",
      "\t CrisisFACTS-008-r6 26\n",
      "\t CrisisFACTS-008-r7 30\n",
      "\t CrisisFACTS-008-r8 72\n",
      "\t CrisisFACTS-008-r9 19\n",
      "\t CrisisFACTS-008-r10 1\n"
     ]
    }
   ],
   "source": [
    "event_request_fact_count_map = {}\n",
    "\n",
    "day_count = 0 \n",
    "total_fact_count = 0\n",
    "for event in facts:\n",
    "    event_name = event[\"event\"]\n",
    "    event_id = event[\"eventID\"]\n",
    "    event_requests = event[\"summaryRequests\"]\n",
    "    event_factsXrequests = event[\"factsByRequest\"]\n",
    "\n",
    "    print(event_id, event_name)\n",
    "    for event_request in event_requests:\n",
    "        req_id = event_request[\"requestID\"]        \n",
    "        this_req_facts = event_factsXrequests[req_id]\n",
    "        fact_count = len(this_req_facts)\n",
    "        fact_collection = [fact[\"fact\"] for fact in this_req_facts]\n",
    "        \n",
    "        print(\"\\t\", req_id, fact_count)\n",
    "        event_request_fact_count_map[req_id] = fact_count\n",
    "        \n",
    "        total_fact_count+=fact_count\n",
    "        day_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee654e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "For each submission, we iterate through each event. For each event, we take the top facts for each day and add them to a running summary for that event. After constructing the full event summary across all days, we use  `rouge` to score the full event summary.\n",
    "\n",
    "NOTE: We do not evaluate daily summaries as Wikipedia does not provide us with daily summaries, only top-level summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "45c9d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = ROUGEScore(\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "835e1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a658b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submissions.extractive/baseline.run1.json.gz --> baseline.run1\n",
      "CrisisFACTS-001 56536\n",
      "CrisisFACTS-002 9043\n",
      "CrisisFACTS-003 26617\n",
      "CrisisFACTS-004 33512\n",
      "CrisisFACTS-005 10305\n",
      "CrisisFACTS-006 18176\n",
      "CrisisFACTS-007 26680\n",
      "CrisisFACTS-008 27691\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rouge1_fmeasure</th>\n",
       "      <td>0.417783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge1_precision</th>\n",
       "      <td>0.334461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge1_recall</th>\n",
       "      <td>0.612103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge2_fmeasure</th>\n",
       "      <td>0.132571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge2_precision</th>\n",
       "      <td>0.106472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge2_recall</th>\n",
       "      <td>0.188862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeL_fmeasure</th>\n",
       "      <td>0.121186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeL_precision</th>\n",
       "      <td>0.095554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeL_recall</th>\n",
       "      <td>0.183059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeLsum_fmeasure</th>\n",
       "      <td>0.392176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeLsum_precision</th>\n",
       "      <td>0.315553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeLsum_recall</th>\n",
       "      <td>0.566986</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        value\n",
       "metric                       \n",
       "rouge1_fmeasure      0.417783\n",
       "rouge1_precision     0.334461\n",
       "rouge1_recall        0.612103\n",
       "rouge2_fmeasure      0.132571\n",
       "rouge2_precision     0.106472\n",
       "rouge2_recall        0.188862\n",
       "rougeL_fmeasure      0.121186\n",
       "rougeL_precision     0.095554\n",
       "rougeL_recall        0.183059\n",
       "rougeLsum_fmeasure   0.392176\n",
       "rougeLsum_precision  0.315553\n",
       "rougeLsum_recall     0.566986"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "submissions.extractive/baseline.run2.json.gz --> baseline.run2\n",
      "CrisisFACTS-001 56670\n",
      "CrisisFACTS-002 8988\n",
      "CrisisFACTS-003 25993\n",
      "CrisisFACTS-004 34649\n",
      "CrisisFACTS-005 9957\n",
      "CrisisFACTS-006 18859\n",
      "CrisisFACTS-007 28536\n",
      "CrisisFACTS-008 26660\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>metric</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rouge1_fmeasure</th>\n",
       "      <td>0.415547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge1_precision</th>\n",
       "      <td>0.332996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge1_recall</th>\n",
       "      <td>0.608375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge2_fmeasure</th>\n",
       "      <td>0.130774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge2_precision</th>\n",
       "      <td>0.105220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rouge2_recall</th>\n",
       "      <td>0.185055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeL_fmeasure</th>\n",
       "      <td>0.119972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeL_precision</th>\n",
       "      <td>0.094622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeL_recall</th>\n",
       "      <td>0.181891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeLsum_fmeasure</th>\n",
       "      <td>0.390892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeLsum_precision</th>\n",
       "      <td>0.314593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rougeLsum_recall</th>\n",
       "      <td>0.565553</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        value\n",
       "metric                       \n",
       "rouge1_fmeasure      0.415547\n",
       "rouge1_precision     0.332996\n",
       "rouge1_recall        0.608375\n",
       "rouge2_fmeasure      0.130774\n",
       "rouge2_precision     0.105220\n",
       "rouge2_recall        0.185055\n",
       "rougeL_fmeasure      0.119972\n",
       "rougeL_precision     0.094622\n",
       "rougeL_recall        0.181891\n",
       "rougeLsum_fmeasure   0.390892\n",
       "rougeLsum_precision  0.314593\n",
       "rougeLsum_recall     0.565553"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take the top-k facts from each run and each event-request pair per run\n",
    "event_request_fact_list = {k:{} for k in event_request_fact_count_map.keys()}\n",
    "for f in glob.glob(\"submissions.*/*.json.gz\"):\n",
    "    \n",
    "    this_run_id = f.partition(\"/\")[-1].replace(\".json.gz\", \"\")\n",
    "    print(f, \"-->\", this_run_id)\n",
    "    \n",
    "    this_run_event_request_facts = {k:[] for k in event_request_fact_count_map.keys()}\n",
    "    with gzip.open(f, \"r\") as in_file:\n",
    "        for line_ in in_file:\n",
    "            line = line_.decode(\"utf8\")\n",
    "            \n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            this_run_event_request_facts[entry[\"requestID\"]].append(entry)\n",
    "            \n",
    "    event_summaries = {s[\"eventID\"]:[] for s in summaries}\n",
    "    for event_request,this_fact_list in this_run_event_request_facts.items():\n",
    "        event_id = event_request.rpartition(\"-\")[0]\n",
    "        \n",
    "        sorted_fact_list = sorted(this_fact_list, key=lambda v: v[\"importance\"], reverse=True)\n",
    "        \n",
    "        this_event_request_k = event_request_fact_count_map[event_request]\n",
    "        this_day_summary = [this_top_fact[\"factText\"] for this_top_fact in sorted_fact_list[:this_event_request_k]]\n",
    "        \n",
    "        event_summaries[event_id] = event_summaries[event_id] + this_day_summary\n",
    "        \n",
    "\n",
    "    ics_dfs = []\n",
    "    wiki_dfs = []\n",
    "    nist_dfs = []\n",
    "    for event in summaries:\n",
    "        event_id = event[\"eventID\"]\n",
    "        \n",
    "        this_submitted_summary = event_summaries[event_id]\n",
    "\n",
    "        this_summary_text = \" \".join(this_submitted_summary)\n",
    "        print(event_id, len(this_summary_text))\n",
    "        \n",
    "        nist_summary = event[\"nist.summary\"]\n",
    "        wiki_summary = event[\"wiki.summary\"]\n",
    "        ics_summary = event.get(\"ics.summary\", \"\")\n",
    "\n",
    "        nist_metric = rouge(this_summary_text, nist_summary)\n",
    "        wiki_metric = rouge(this_summary_text, wiki_summary)\n",
    "        ics_metric = rouge(this_summary_text, ics_summary)\n",
    "        \n",
    "        this_ics_df = pd.DataFrame([{\"metric\":k, \"value\":v.item(), \"event\": event_id} for k,v in ics_metric.items()])\n",
    "        this_wiki_df = pd.DataFrame([{\"metric\":k, \"value\":v.item(), \"event\": event_id} for k,v in wiki_metric.items()])\n",
    "        this_nist_df = pd.DataFrame([{\"metric\":k, \"value\":v.item(), \"event\": event_id} for k,v in nist_metric.items()])\n",
    "        \n",
    "        ics_dfs.append(this_ics_df)\n",
    "        wiki_dfs.append(this_wiki_df)\n",
    "        nist_dfs.append(this_nist_df)\n",
    "        \n",
    "    full_ics_df = pd.concat(ics_dfs)\n",
    "    full_wiki_df = pd.concat(wiki_dfs)\n",
    "    full_nist_df = pd.concat(nist_dfs)\n",
    "    \n",
    "    submission_metrics[this_run_id] = {\n",
    "        \"ics\": full_ics_df,\n",
    "        \"wiki\": full_wiki_df,\n",
    "        \"nist\": full_nist_df,\n",
    "    }\n",
    "    \n",
    "    display(full_nist_df.groupby(\"metric\").mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc93d833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1542404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir evaluation.output.rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aaeee4",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Save the evaluation for each participant system to its own file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "13c81f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline.run1\n",
      "baseline.run2\n"
     ]
    }
   ],
   "source": [
    "all_runs = []\n",
    "for k,v in submission_metrics.items():\n",
    "    print(k)\n",
    "    \n",
    "    stackable = []\n",
    "    for comparator,ldf in v.items():\n",
    "        stackable_ldf = ldf.copy()\n",
    "        stackable_ldf[\"target.summary\"] = comparator\n",
    "\n",
    "        stackable.append(stackable_ldf)\n",
    "\n",
    "    this_run_df = pd.concat(stackable)\n",
    "    this_run_df[\"run\"] = k\n",
    "    \n",
    "    all_runs.append(this_run_df)\n",
    "    this_run_df.to_csv(\"evaluation.output.rouge/%s.csv\" % k, index=False)\n",
    "    \n",
    "all_runs_df = pd.concat(all_runs)\n",
    "all_runs_df.to_csv(\"evaluation.output.rouge/all_runs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8ccb3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cd743bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "baseline.run1\n",
      "baseline.run2\n",
      "baseline.run1\n",
      "baseline.run2\n",
      "baseline.run1\n",
      "baseline.run2\n"
     ]
    }
   ],
   "source": [
    "target_summaries = {}\n",
    "for target in [\"ics\", \"wiki\", \"nist\"]:\n",
    "    this_target_df = all_runs_df[all_runs_df[\"target.summary\"] == target]\n",
    "    \n",
    "    index = []\n",
    "    rows = []\n",
    "    for run_name,group in this_target_df.groupby(\"run\"):\n",
    "        print(run_name)\n",
    "        this_row = group.pivot(\"event\", \"metric\", \"value\").mean()\n",
    "        rows.append(this_row)\n",
    "        index.append(run_name)\n",
    "\n",
    "    summary_df = pd.DataFrame(rows, index=index)[[\n",
    "        \"rouge2_fmeasure\", \n",
    "    ]]\n",
    "\n",
    "    final_df = summary_df.sort_values(by=\"rouge2_fmeasure\", ascending=False)\n",
    "    final_df.to_csv(\"evaluation.output.rouge/%s.summary.csv\" % target)\n",
    "    \n",
    "    target_summaries[target] = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a547fcbe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
