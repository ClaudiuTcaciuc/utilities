{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "891c1805",
   "metadata": {},
   "source": [
    "# ROUGE-Style Evaluation\n",
    "\n",
    "__Author__: Cody Buntain (cbuntain@umd.edu)\n",
    "\n",
    "## Description\n",
    "\n",
    "For automated evaluation in CrisisFACTS, we compare participant-system summaries to three additional sources of event summaries:\n",
    "\n",
    "1. Wikipedia - A simple summary of each event, though we expect these summaries are not massively useful for situational awareness, attention support, or decision making.\n",
    "\n",
    "2. ICS 209 Archive - A dataset of real daily hazard reports, gathered from Lise St. Denis. This data comes from a pre-release version of their updated NIMS database.\n",
    "    \n",
    "3. NIST Assessor Summaries - A dataset of event summaries generated by NIST assessors, where CrisisFACTS coordinators asked NIST assessors to identify and timestamp important facts from each event.\n",
    "\n",
    "We use ROUGE score to compare the top-k most important facts from each participant system to each of the above summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "44a96521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import glob\n",
    "import gzip\n",
    "\n",
    "import scipy.stats\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c556aa5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.text.rouge import ROUGEScore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41c1a5e9",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Gold summaries, generated by `00-CreateMultiSummaries` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "535b363b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with gzip.open(\"gold.summaries.json.gz\", \"rb\") as in_file:\n",
    "    summaries = json.load(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4111b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"CrisisFACTs-2022.facts.json\", \"r\") as in_file:\n",
    "    facts = json.load(in_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a21f1c7",
   "metadata": {},
   "source": [
    "We use the CrisisFACTS 2022 fact list from NIST assessors to determine the number of facts *per day*.\n",
    "\n",
    "We use this \"depth\" to take the top most important facts from each participant system for that day. \n",
    "\n",
    "E.g., if a system returns 1000 facts, but the NIST assessor only found 417 facts for that event-day pair, we take the top 417most important facts, as ranked by the participant system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf72124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_request_fact_count_map = {}\n",
    "\n",
    "day_count = 0 \n",
    "total_fact_count = 0\n",
    "for event in facts:\n",
    "    event_name = event[\"event\"]\n",
    "    event_id = event[\"eventID\"]\n",
    "    event_requests = event[\"summaryRequests\"]\n",
    "    event_factsXrequests = event[\"factsByRequest\"]\n",
    "\n",
    "    print(event_id, event_name)\n",
    "    for event_request in event_requests:\n",
    "        req_id = event_request[\"requestID\"]        \n",
    "        this_req_facts = event_factsXrequests[req_id]\n",
    "        fact_count = len(this_req_facts)\n",
    "        fact_collection = [fact[\"fact\"] for fact in this_req_facts]\n",
    "        \n",
    "        print(\"\\t\", req_id, fact_count)\n",
    "        event_request_fact_count_map[req_id] = fact_count\n",
    "        \n",
    "        total_fact_count+=fact_count\n",
    "        day_count+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee654e",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "For each submission, we iterate through each event. For each event, we take the top facts for each day and add them to a running summary for that event. After constructing the full event summary across all days, we use  `rouge` to score the full event summary.\n",
    "\n",
    "NOTE: We do not evaluate daily summaries as Wikipedia does not provide us with daily summaries, only top-level summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c9d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = ROUGEScore(\n",
    "    use_stemmer=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835e1d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_metrics = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a658b39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Take the top-k facts from each run and each event-request pair per run\n",
    "event_request_fact_list = {k:{} for k in event_request_fact_count_map.keys()}\n",
    "for f in glob.glob(\"submissions.*/*.json.gz\"):\n",
    "    \n",
    "    this_run_id = f.partition(\"/\")[-1].replace(\".json.gz\", \"\")\n",
    "    print(f, \"-->\", this_run_id)\n",
    "    \n",
    "    this_run_event_request_facts = {k:[] for k in event_request_fact_count_map.keys()}\n",
    "    with gzip.open(f, \"r\") as in_file:\n",
    "        for line_ in in_file:\n",
    "            line = line_.decode(\"utf8\")\n",
    "            \n",
    "            entry = json.loads(line)\n",
    "            \n",
    "            this_run_event_request_facts[entry[\"requestID\"]].append(entry)\n",
    "            \n",
    "    event_summaries = {s[\"eventID\"]:[] for s in summaries}\n",
    "    for event_request,this_fact_list in this_run_event_request_facts.items():\n",
    "        event_id = event_request.rpartition(\"-\")[0]\n",
    "        \n",
    "        sorted_fact_list = sorted(this_fact_list, key=lambda v: v[\"importance\"], reverse=True)\n",
    "        \n",
    "        this_event_request_k = event_request_fact_count_map[event_request]\n",
    "        this_day_summary = [this_top_fact[\"factText\"] for this_top_fact in sorted_fact_list[:this_event_request_k]]\n",
    "        \n",
    "        event_summaries[event_id] = event_summaries[event_id] + this_day_summary\n",
    "        \n",
    "\n",
    "    ics_dfs = []\n",
    "    wiki_dfs = []\n",
    "    nist_dfs = []\n",
    "    for event in summaries:\n",
    "        event_id = event[\"eventID\"]\n",
    "        \n",
    "        this_submitted_summary = event_summaries[event_id]\n",
    "\n",
    "        this_summary_text = \" \".join(this_submitted_summary)\n",
    "        print(event_id, len(this_summary_text))\n",
    "        \n",
    "        nist_summary = event[\"nist.summary\"]\n",
    "        wiki_summary = event[\"wiki.summary\"]\n",
    "        ics_summary = event.get(\"ics.summary\", \"\")\n",
    "\n",
    "        nist_metric = rouge(this_summary_text, nist_summary)\n",
    "        wiki_metric = rouge(this_summary_text, wiki_summary)\n",
    "        ics_metric = rouge(this_summary_text, ics_summary)\n",
    "        \n",
    "        this_ics_df = pd.DataFrame([{\"metric\":k, \"value\":v.item(), \"event\": event_id} for k,v in ics_metric.items()])\n",
    "        this_wiki_df = pd.DataFrame([{\"metric\":k, \"value\":v.item(), \"event\": event_id} for k,v in wiki_metric.items()])\n",
    "        this_nist_df = pd.DataFrame([{\"metric\":k, \"value\":v.item(), \"event\": event_id} for k,v in nist_metric.items()])\n",
    "        \n",
    "        ics_dfs.append(this_ics_df)\n",
    "        wiki_dfs.append(this_wiki_df)\n",
    "        nist_dfs.append(this_nist_df)\n",
    "        \n",
    "    full_ics_df = pd.concat(ics_dfs)\n",
    "    full_wiki_df = pd.concat(wiki_dfs)\n",
    "    full_nist_df = pd.concat(nist_dfs)\n",
    "    \n",
    "    submission_metrics[this_run_id] = {\n",
    "        \"ics\": full_ics_df,\n",
    "        \"wiki\": full_wiki_df,\n",
    "        \"nist\": full_nist_df,\n",
    "    }\n",
    "    \n",
    "    display(full_nist_df.groupby(\"metric\").mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc93d833",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1542404c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir evaluation.output.rouge"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41aaeee4",
   "metadata": {},
   "source": [
    "<hr>\n",
    "Save the evaluation for each participant system to its own file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c81f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_runs = []\n",
    "for k,v in submission_metrics.items():\n",
    "    print(k)\n",
    "    \n",
    "    stackable = []\n",
    "    for comparator,ldf in v.items():\n",
    "        stackable_ldf = ldf.copy()\n",
    "        stackable_ldf[\"target.summary\"] = comparator\n",
    "\n",
    "        stackable.append(stackable_ldf)\n",
    "\n",
    "    this_run_df = pd.concat(stackable)\n",
    "    this_run_df[\"run\"] = k\n",
    "    \n",
    "    all_runs.append(this_run_df)\n",
    "    this_run_df.to_csv(\"evaluation.output.rouge/%s.csv\" % k, index=False)\n",
    "    \n",
    "all_runs_df = pd.concat(all_runs)\n",
    "all_runs_df.to_csv(\"evaluation.output.rouge/all_runs.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8ccb3e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd743bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_summaries = {}\n",
    "for target in [\"ics\", \"wiki\", \"nist\"]:\n",
    "    this_target_df = all_runs_df[all_runs_df[\"target.summary\"] == target]\n",
    "    \n",
    "    index = []\n",
    "    rows = []\n",
    "    for run_name,group in this_target_df.groupby(\"run\"):\n",
    "        print(run_name)\n",
    "        this_row = group.pivot(\"event\", \"metric\", \"value\").mean()\n",
    "        rows.append(this_row)\n",
    "        index.append(run_name)\n",
    "\n",
    "    summary_df = pd.DataFrame(rows, index=index)[[\n",
    "        \"f1\", \n",
    "    ]]\n",
    "\n",
    "    final_df = summary_df.sort_values(by=\"f1\", ascending=False)\n",
    "    final_df.to_csv(\"evaluation.output.rouge/%s.summary.csv\" % target)\n",
    "    \n",
    "    target_summaries[target] = final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2e75c09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
